{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6cec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4bce92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset stsb_multi_mt (C:/Users/k0nv1ct/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513748e658fb4ce0a13b7487235895a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 3) (1379, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                          sentence2  similarity_score  \n",
       "0                      A girl is brushing her hair.               2.5  \n",
       "1  A group of boys are playing soccer on the beach.               3.6  \n",
       "2           A woman measures another woman's ankle.               5.0  \n",
       "3                      A man is slicing a cucumber.               4.2  \n",
       "4                      A man is playing a keyboard.               1.5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the English STSB dataset\n",
    "stsb_dataset = load_dataset('stsb_multi_mt', 'en')\n",
    "stsb_train = pd.DataFrame(stsb_dataset['train'])\n",
    "stsb_test = pd.DataFrame(stsb_dataset['test'])\n",
    "\n",
    "# Check loaded data\n",
    "print(stsb_train.shape, stsb_test.shape)\n",
    "stsb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76fe3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\k0nv1ct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ce5099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\k0nv1ct\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a741f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1379 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/1379 [00:00<02:26,  9.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 10/1379 [00:00<00:36, 37.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 22/1379 [00:00<00:20, 66.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 34/1379 [00:00<00:16, 83.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 47/1379 [00:00<00:13, 97.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 60/1379 [00:00<00:12, 105.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 73/1379 [00:00<00:11, 111.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 86/1379 [00:00<00:11, 115.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 98/1379 [00:01<00:11, 114.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 110/1379 [00:01<00:11, 113.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 122/1379 [00:01<00:11, 113.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 135/1379 [00:01<00:10, 118.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 148/1379 [00:01<00:10, 120.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 162/1379 [00:01<00:09, 124.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 175/1379 [00:01<00:09, 122.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 188/1379 [00:01<00:09, 124.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 202/1379 [00:01<00:09, 126.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 215/1379 [00:01<00:09, 127.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 228/1379 [00:02<00:09, 125.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 241/1379 [00:02<00:09, 124.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 254/1379 [00:02<00:08, 125.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 267/1379 [00:02<00:09, 122.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 280/1379 [00:02<00:08, 122.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 293/1379 [00:02<00:08, 121.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 306/1379 [00:02<00:09, 116.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 318/1379 [00:02<00:09, 117.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 330/1379 [00:02<00:08, 116.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 343/1379 [00:03<00:08, 119.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 356/1379 [00:03<00:08, 120.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 369/1379 [00:03<00:08, 122.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 383/1379 [00:03<00:07, 125.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 396/1379 [00:03<00:07, 123.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 409/1379 [00:03<00:07, 123.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 422/1379 [00:03<00:07, 123.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 435/1379 [00:03<00:07, 123.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 448/1379 [00:03<00:07, 123.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 461/1379 [00:04<00:07, 122.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 474/1379 [00:04<00:07, 123.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 487/1379 [00:04<00:07, 119.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 501/1379 [00:04<00:07, 123.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 514/1379 [00:04<00:07, 123.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 527/1379 [00:04<00:06, 124.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 540/1379 [00:04<00:06, 121.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 553/1379 [00:04<00:06, 123.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 567/1379 [00:04<00:06, 125.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 580/1379 [00:04<00:06, 126.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 593/1379 [00:05<00:06, 126.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 606/1379 [00:05<00:06, 125.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 619/1379 [00:05<00:06, 125.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 632/1379 [00:05<00:06, 124.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 645/1379 [00:05<00:05, 123.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 658/1379 [00:05<00:05, 123.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 671/1379 [00:05<00:05, 123.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 684/1379 [00:05<00:05, 122.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 697/1379 [00:05<00:05, 124.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 711/1379 [00:06<00:05, 127.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 724/1379 [00:06<00:05, 126.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 738/1379 [00:06<00:05, 127.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 751/1379 [00:06<00:04, 128.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 764/1379 [00:06<00:04, 125.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 778/1379 [00:06<00:04, 126.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 792/1379 [00:06<00:04, 128.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 805/1379 [00:06<00:04, 124.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 818/1379 [00:06<00:04, 123.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 831/1379 [00:06<00:04, 122.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 844/1379 [00:07<00:04, 124.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 857/1379 [00:07<00:04, 125.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 870/1379 [00:07<00:04, 125.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 883/1379 [00:07<00:04, 118.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 895/1379 [00:07<00:04, 103.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 906/1379 [00:07<00:04, 96.61it/s] \u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 916/1379 [00:07<00:05, 92.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 926/1379 [00:07<00:05, 88.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 935/1379 [00:08<00:05, 88.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 945/1379 [00:08<00:04, 90.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 955/1379 [00:08<00:04, 88.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 964/1379 [00:08<00:04, 88.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 974/1379 [00:08<00:04, 89.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 983/1379 [00:08<00:04, 88.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 993/1379 [00:08<00:04, 90.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1003/1379 [00:08<00:04, 90.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1013/1379 [00:08<00:03, 93.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 1023/1379 [00:09<00:03, 92.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 1033/1379 [00:09<00:03, 90.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1043/1379 [00:09<00:03, 90.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 1053/1379 [00:09<00:03, 90.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1063/1379 [00:09<00:03, 90.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1073/1379 [00:09<00:03, 91.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 1083/1379 [00:09<00:03, 91.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1093/1379 [00:09<00:03, 92.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 1103/1379 [00:09<00:02, 93.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 1113/1379 [00:09<00:02, 94.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 1123/1379 [00:10<00:02, 92.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1133/1379 [00:10<00:02, 93.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 1146/1379 [00:10<00:02, 103.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1159/1379 [00:10<00:02, 109.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 1172/1379 [00:10<00:01, 114.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 1185/1379 [00:10<00:01, 118.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1197/1379 [00:10<00:01, 117.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 1210/1379 [00:10<00:01, 120.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▊ | 1223/1379 [00:10<00:01, 120.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 1236/1379 [00:11<00:01, 123.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 1250/1379 [00:11<00:01, 126.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1263/1379 [00:11<00:00, 126.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 1276/1379 [00:11<00:00, 125.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▎| 1290/1379 [00:11<00:00, 127.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 1303/1379 [00:11<00:00, 124.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 1317/1379 [00:11<00:00, 127.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 1330/1379 [00:11<00:00, 127.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 1344/1379 [00:11<00:00, 128.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1357/1379 [00:11<00:00, 128.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1379/1379 [00:12<00:00, 113.36it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#Jaccaed Similarity Score on Hugging face STSB dataset\n",
    "import textdistance\n",
    "\n",
    "def jaccard_sim(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    return textdistance.jaccard.normalized_similarity(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Jaccard Similarity\n",
    "stsb_test['Jaccard_score'] = stsb_test.progress_apply(jaccard_sim, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e71c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUnction to replicate any article from wikipedia for similarity test\n",
    "def fetch_text(link):\n",
    "    url = urllib.request.urlopen(link).read()\n",
    "\n",
    "    soup = bs.BeautifulSoup(url, 'lxml')\n",
    "\n",
    "    text = \"\"\n",
    "    for para in soup.find_all('p'):\n",
    "        text += para.text\n",
    "    \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e94e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url of the articles and their respective variables\n",
    "url = ['https://en.wikipedia.org/wiki/Baba_Jan_(politician)','https://en.wikipedia.org/wiki/CERN',\n",
    "       'https://en.wikipedia.org/wiki/Large_Hadron_Collider']\n",
    "doc1 = fetch_text(url[0])\n",
    "doc2 = fetch_text(url[1])\n",
    "doc3 = fetch_text(url[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214e1cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 120\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the fetcjed data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "tokens = sent_tokenize(doc2)\n",
    "for line in tokens:\n",
    "    file_docs.append(line)\n",
    "\n",
    "print(\"Number of Sentences:\",len(file_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdd709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing again to put into a dataframe\n",
    "from nltk import tokenize\n",
    "arr_doc2 = tokenize.sent_tokenize(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198249c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting sentences into dataframe\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(arr_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c611ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting column name\n",
    "df1.columns = ['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "245dc3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe of the secont article\n",
    "from nltk import tokenize\n",
    "arr_doc3 = tokenize.sent_tokenize(doc3)\n",
    "df3 = pd.DataFrame(arr_doc3)\n",
    "df3.columns = ['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6d097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final data frame after merging both the article scrapped from wikipedia\n",
    "final = pd.concat([df1, df3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "674f9f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence1    156\n",
       "sentence2      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea4fe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a279a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(': 0, ')': 1, ',': 2, '.': 3, '/sɜːrn/': 4, ':': 5, ';': 6, '[': 7, ']': 8, 'an': 9, 'as': 10, 'cern': 11, 'conseil': 12, 'european': 13, 'européen': 14, 'for': 15, 'french': 16, 'in': 17, 'intergovernmental': 18, 'is': 19, 'known': 20, 'la': 21, 'laboratory': 22, 'largest': 23, 'nuclear': 24, 'nucléaire': 25, 'operates': 26, 'organization': 27, 'particle': 28, 'physics': 29, 'pour': 30, 'pronunciation': 31, 'recherche': 32, 'research': 33, 'sɛʁn': 34, 'that': 35, 'the': 36, 'world': 37, '\\u200b': 38, 'a': 39, 'based': 40, 'border': 41, 'established': 42, 'france–switzerland': 43, 'geneva': 44, 'it': 45, 'northwestern': 46, 'of': 47, 'on': 48, 'suburb': 49, 'admitted': 50, 'and': 51, 'comprises': 52, 'country': 53, 'currently': 54, 'full': 55, 'holding': 56, 'israel': 57, 'member': 58, 'membership': 59, 'non-european': 60, 'only': 61, 'states': 62, 'assembly': 63, 'general': 64, 'nations': 65, 'observer': 66, 'official': 67, 'united': 68, 'about': 69, 'acronym': 70, 'administrative': 71, 'also': 72, 'countries': 73, 'from': 74, 'had': 75, 'hosted': 76, 'institutions': 77, 'members': 78, 'more': 79, 'refer': 80, 'scientific': 81, 'staff': 82, 'technical': 83, 'than': 84, 'to': 85, 'used': 86, 'users': 87, 'data': 88, 'generated': 89, 'petabytes': 90, \"'s\": 91, 'accelerators': 92, 'at': 93, 'been': 94, 'collaborations': 95, 'consequently': 96, 'constructed': 97, 'experiments': 98, 'function': 99, 'have': 100, 'high-energy': 101, 'infrastructure': 102, 'international': 103, 'main': 104, 'needed': 105, 'numerous': 106, 'other': 107, 'provide': 108, 'through': 109, '—': 110, 'collider': 111, 'hadron': 112, 'highest-energy': 113, 'large': 114, 'lhc': 115, 'site': 116, 'analyze': 117, 'computing': 118, 'events': 119, 'facility': 120, 'hosts': 121, 'meyrin': 122, 'primarily': 123, 'simulate': 124, 'store': 125, 'well': 126, 'which': 127, 'access': 128, 'area': 129, 'facilities': 130, 'has': 131, 'historically': 132, 'hub': 133, 'lab': 134, 'major': 135, 'network': 136, 'remote': 137, 'require': 138, 'researchers': 139, 'these': 140, 'wide': 141, 'birthplace': 142, 'web': 143, 'by': 144, 'convention': 145, 'establishing': 146, 'europe': 147, 'ratified': 148, 'september': 149, 'was': 150, 'western': 151, \"'\": 152, \"'european\": 153, 'building': 154, 'council': 155, 'governments': 156, 'originally': 157, 'provisional': 158, 'represented': 159, 'words': 160, 'before': 161, 'bohr': 162, 'copenhagen': 163, 'direction': 164, 'during': 165, 'early': 166, 'its': 167, 'moving': 168, 'niels': 169, 'present': 170, 'under': 171, 'university': 172, 'worked': 173, 'years': 174, 'after': 175, 'changed': 176, 'current': 177, 'dissolved': 178, 'européenne': 179, 'even': 180, 'name': 181, 'new': 182, 'organisation': 183, 'retained': 184, 'though': 185, \"''\": 186, '``': 187, 'abbreviation': 188, 'according': 189, 'awkward': 190, 'be': 191, 'become': 192, 'could': 193, 'director': 194, 'former': 195, 'heisenberg': 196, 'if': 197, 'kowarski': 198, 'lew': 199, 'not': 200, 'oern': 201, 'said': 202, 'still': 203, 'this': 204, 'werner': 205, 'when': 206, 'benjamin': 207, 'first': 208, 'lockspeiser': 209, 'president': 210, 'sir': 211, 'amaldi': 212, 'bloch': 213, 'director-general': 214, 'edoardo': 215, 'felix': 216, 'operations': 217, 'secretary': 218, 'stages': 219, 'were': 220, 'while': 221, 'applied': 222, 'atomic': 223, 'between': 224, 'but': 225, 'concerned': 226, 'devoted': 227, 'higher-energy': 228, 'interactions': 229, 'mainly': 230, 'nuclei': 231, 'particles': 232, 'soon': 233, 'study': 234, 'subatomic': 235, 'with': 236, 'being': 237, 'better': 238, 'commonly': 239, 'des': 240, 'describes': 241, 'laboratoire': 242, 'operated': 243, 'particules': 244, 'performed': 245, 'physique': 246, 'referred': 247, 'there': 248, 'therefore': 249, 'citation': 250, 'july': 251, 'june': 252, 'paris': 253, 'place': 254, 'ratification': 255, 'session': 256, 'signed': 257, 'sixth': 258, 'subject': 259, 'took': 260, '–': 261, 'belgium': 262, 'denmark': 263, 'federal': 264, 'founding': 265, 'france': 266, 'germany': 267, 'gradually': 268, 'greece': 269, 'italy': 270, 'kingdom': 271, 'netherlands': 272, 'norway': 273, 'republic': 274, 'sweden': 275, 'switzerland': 276, 'yugoslavia': 277, 'achievements': 278, 'important': 279, 'made': 280, 'several': 281, 'attention': 282, 'attracted': 283, 'collaboration': 284, 'detection': 285, 'faster-than-light': 286, 'include': 287, 'media': 288, 'neutrinos': 289, 'opera': 290, 'possibly': 291, 'reported': 292, 'they': 293, 'cable': 294, 'connected': 295, 'due': 296, 'flawed': 297, 'further': 298, 'gps': 299, 'incorrectly': 300, 'results': 301, 'showed': 302, 'synchronization': 303, 'tests': 304, 'awarded': 305, 'bosons': 306, 'carlo': 307, 'der': 308, 'developments': 309, 'discoveries': 310, 'meer': 311, 'nobel': 312, 'prize': 313, 'resulted': 314, 'rubbia': 315, 'simon': 316, 'van': 317, 'w': 318, 'z': 319, 'chamber': 320, 'charpak': 321, 'detectors': 322, 'development': 323, 'georges': 324, 'his': 325, 'invention': 326, 'multiwire': 327, 'particular': 328, 'proportional': 329, 'researcher': 330, 'boson': 331, 'description': 332, 'englert': 333, 'found': 334, 'françois': 335, 'higgs': 336, 'mechanism': 337, 'peter': 338, 'theoretical': 339, 'year': 340, 'began': 341, 'berners-lee': 342, 'cailliau': 343, 'enquire': 344, 'initiated': 345, 'named': 346, 'project': 347, 'robert': 348, 'tim': 349, 'association': 350, 'contributions': 351, 'honoured': 352, 'jointly': 353, 'machinery': 354, 'their': 355, 'concept': 356, 'facilitate': 357, 'hypertext': 358, 'information': 359, 'intended': 360, 'sharing': 361, 'activated': 362, 'website': 363, 'announced': 364, 'anyone': 365, 'april': 366, 'free': 367, 'would': 368, 'consortium': 369, 'copy': 370, 'created': 371, 'document': 372, 'historical': 373, 'original': 374, 'published': 375, 'webpage': 376, 'beginning': 377, 'e-science': 378, 'egee': 379, 'enabling': 380, 'grid': 381, 'grids': 382, 'hosting': 383, 'including': 384, 'internet': 385, 'introduction': 386, 'pioneered': 387, 'prior': 388, 'projects': 389, 'recently': 390, 's.': 391, 'technology': 392, 'cixp': 393, 'exchange': 394, 'one': 395, 'point': 396, 'points': 397, 'two': 398, 'employs': 399, 'engineers': 400, 'physicists': 401, 'technicians': 402, 'ten': 403, 'times': 404, 'update': 405, 'additional': 406, 'decelerators': 407, 'seven': 408, 'small': 409, 'some': 410, 'accelerator': 411, 'accelerators/decelerators': 412, 'beams': 413, 'chain': 414, 'decrease': 415, 'delivering': 416, 'each': 417, 'energy': 418, 'increases': 419, 'machine': 420, 'naturally': 421, 'next': 422, 'or': 423, 'powerful': 424, 'them': 425, 'active': 426, 'activities': 427, 'are': 428, 'involve': 429, 'machines': 430, 'many': 431, 'operating': 432, 'cooperation': 433, 'large-scale': 434, 'represents': 435, 'worldwide': 436, 'airport': 437, 'jura': 438, 'located': 439, 'metres': 440, 'mountains': 441, 'nearby': 442, 'region': 443, 'tunnel': 444, 'underground': 445, 'length': 446, 'majority': 447, 'side': 448, 'circular': 449, 'circumference': 450, 'down': 451, 'electron–positron': 452, 'km': 453, 'lep': 454, 'november': 455, 'occupied': 456, 'previously': 457, 'shut': 458, 'uses': 459, 'complexes': 460, 'existing': 461, 'injected': 462, 'into': 463, 'ions': 464, 'lead': 465, 'pre-accelerate': 466, 'protons': 467, 'ps/sps': 468, 'then': 469, 'alice': 470, 'along': 471, 'aspect': 472, 'atlas': 473, 'cms': 474, 'collisions': 475, 'different': 476, 'eight': 477, 'faser': 478, 'lhcb': 479, 'lhcf': 480, 'moedal': 481, 'studies': 482, 'technologies': 483, 'totem': 484, 'construction': 485, 'effort': 486, 'engineering': 487, 'extraordinary': 488, 'required': 489, 'cavern': 490, 'crane': 491, 'detector': 492, 'example': 493, 'lower': 494, 'nearly': 495, 'piece': 496, 'pieces': 497, 'rented': 498, 'since': 499, 'special': 500, 'tons': 501, 'weighed': 502, 'approximately': 503, 'gmt': 504, 'lowered': 505, 'magnets': 506, 'march': 507, 'necessary': 508, 'shaft': 509, 'around': 510, 'begun': 511, 'distributed': 512, 'generate': 513, 'laboratories': 514, 'making': 515, 'processing': 516, 'quantities': 517, 'specialized': 518, 'streams': 519, 'use': 520, 'vast': 521, 'across': 522, 'mb/s': 523, 'sites': 524, 'streamed': 525, 'successfully': 526, 'trial': 527, 'august': 528, 'initial': 529, 'beam': 530, 'because': 531, 'circulated': 532, 'connection': 533, 'days': 534, 'entire': 535, 'failed': 536, 'faulty': 537, 'later': 538, 'magnet': 539, 'repairs': 540, 'stopped': 541, 'system': 542, 'circulating': 543, 'operation': 544, 'resumed': 545, 'teraelectronvolts': 546, 'tev': 547, 'challenge': 548, 'line': 549, 'smashed': 550, 'so': 551, 'try': 552, 'up': 553, 'atlantic': 554, 'firing': 555, 'getting': 556, 'hit': 557, 'like': 558, 'myers': 559, 'needles': 560, 'steve': 561, 'collided': 562, 'proton': 563, 'collision': 564, 'per': 565, 'resulting': 566, 'discovery': 567, 'expected': 568, 'however': 569, 'just': 570, 'start': 571, 'what': 572, 'ended': 573, 'experimental': 574, 'period': 575, 'revved': 576, 'starting': 577, 'confirmed': 578, 'scientists': 579, 'sub-atomic': 580, 'allowed': 581, 'conclude': 582, 'measurements': 583, 'newly': 584, 'connections': 585, 'deactivated': 586, 'electrical': 587, 'inside': 588, 'maintenance': 589, 'strengthen': 590, 'two-year': 591, 'upgrades': 592, 'consolidation': 593, 'restarted': 594, 'run': 595, 'second': 596, 'ramp': 597, 'record-breaking': 598, 'design': 599, 'exceeded': 600, 'rate': 601, 'time': 602, 'end': 603, 'shutdown': 604, 'called': 605, 'high': 606, 'hl-lhc': 607, 'luminosity': 608, 'october': 609, 'on-going': 610, 'upgrade': 611, 'higher': 612, 'magnitude': 613, 'order': 614, 'see': 615, 'should': 616, 'upgraded': 617, 'part': 618, 'receiving': 619, 'subsystems': 620, 'among': 621, 'decommissioned': 622, 'injector': 623, 'linac': 624, 'linear': 625, 'replaced': 626, 'work': 627, 'acceleration': 628, 'clic': 629, 'concepts': 630, 'electron-positron': 631, 'future': 632, 'groups': 633, 'increase': 634, 'investigating': 635, 'larger': 636, 'version': 637, 'alongside': 638, 'built': 639, 'extended': 640, 'smaller': 641, 'span': 642, 'west': 643, 'apart': 644, 'jurisdiction': 645, 'marker': 646, 'no': 647, 'obvious': 648, 'stones': 649, 'swiss': 650, 'within': 651, 'almost': 652, 'buried': 653, 'entirely': 654, 'farmland': 655, 'invisible': 656, 'lep/lhc': 657, 'mostly': 658, 'outside': 659, 'sps': 660, 'surface': 661, 'tunnels': 662, 'associated': 663, 'buildings': 664, 'colliders': 665, 'cryogenic': 666, 'either': 667, 'location': 668, 'operate': 669, 'plants': 670, 'shafts': 671, 'such': 672, 'various': 673, 'level': 674, 'same': 675, 'although': 676, 'ancillary': 677, 'three': 678, 'non-collider': 679, 'north': 680, 'prévessin': 681, 'station': 682, 'target': 683, 'latter': 684, 'ones': 685, 'ua': 686, 'most': 687, 'numbered': 688, 'officially': 689, 'where': 690, 'bebc': 691, 'big': 692, 'bubble': 693, 'charmed': 694, 'examine': 695, 'experiment': 696, 'looking': 697, 'na': 698, 'neutrino': 699, 'production': 700, 'so-called': 701, 'wa': 702, 'considered': 703, 'i.e': 704, 'situated': 705, 'creation': 706, 'famous': 707, 'pauli': 708, 'pushed': 709, 'roads': 710, 'who': 711, 'wolfgang': 712, 'albert': 713, 'einstein': 714, 'feynman': 715, 'names': 716, 'notable': 717, 'richard': 718, 'accepted': 719, 'foundation': 720, 'regularly': 721, 'accession': 722, 'all': 723, 'continuously': 724, 'except': 725, 'remained': 726, 'spain': 727, 'joined': 728, 'rejoined': 729, 'withdrew': 730, 'quit': 731, 'becoming': 732, 'january': 733, 'budget': 734, 'computed': 735, 'gdp': 736, '-': 737, 'agreements': 738, 'aid': 739, 'animated': 740, 'associate': 741, 'austria': 742, 'borders': 743, 'bulgaria': 744, 'can': 745, 'candidates': 746, 'change': 747, 'changes': 748, 'co-operation': 749, 'contacts': 750, 'czech': 751, 'dates': 752, 'diplomacy': 753, 'finland': 754, 'following': 755, 'founded': 756, 'hungary': 757, 'involved': 758, 'join': 759, 'joins': 760, 'leaves': 761, 'map': 762, 'non-member': 763, 'observers': 764, 'organizations': 765, 'poland': 766, 'portugal': 767, 'post': 768, 'programmes': 769, 're-joins': 770, 'reunified': 771, 'science': 772, 'showing': 773, 'slovakia': 774, 'status': 775, 'until': 776, 'and/or': 777, 'institutes': 778, 'links': 779, 'number': 780, 'accessible': 781, 'below': 782, 'contains': 783, 'creating': 784, 'focuses': 785, 'knowledge': 786, 'list': 787, 'model': 788, 'movement': 789, 'open': 790, 'openly': 791, 'processes': 792, 'tools': 793, 'areas': 794, 'components': 795, 'digital': 796, 'formation': 797, 'hardware': 798, 'licenses': 799, 'preservation': 800, 'primary': 801, 'reproducible': 802, 'software': 803, 'source': 804, 'towards': 805, 'working': 806, 'available': 807, 'developed': 808, 'documents': 809, 'enable': 810, 'generally': 811, 'indicated': 812, 'policies': 813, 'promote': 814, 'authors': 815, 'endorsed': 816, 'ensures': 817, 'four': 818, 'gold': 819, 'policy': 820, 'publications': 821, 'will': 822, 'addressing': 823, 'collected': 824, 'complements': 825, 'embargo': 826, 'public': 827, 'release': 828, 'suitable': 829, 'guidelines': 830, 'implemented': 831, 'individually': 832, 'own': 833, 'reuse': 834, 'updated': 835, 'adopted': 836, 'affirmed': 837, 'authorities': 838, 'community': 839, 'consensus': 840, 'cornerstone': 841, 'decision-making': 842, 'emerging': 843, 'field': 844, 'forms': 845, 'help': 846, 'implement': 847, 'landscape': 848, 'last': 849, 'mandated': 850, 'publicly-funded': 851, 'relevant': 852, 'role': 853, 'shape': 854, 'stating': 855, 'strategy': 856, 'strongly': 857, '“': 858, '”': 859, 'beyond': 860, 'guide': 861, 'services': 862, 'variety': 863, 'articles': 864, 'convert': 865, 'cooperative': 866, 'global': 867, 'publishing': 868, 'scoap': 869, 'sponsoring': 870, '+': 871, 'collectively': 872, 'discipline': 873, 'journals': 874, 'leading': 875, 'libraries': 876, 'partnership': 877, 'case': 878, 'cern-based': 879, 'core': 880, 'depending': 881, 'etc': 882, 'hepdata': 883, 'inspire': 884, 'multimedia': 885, 'portal': 886, 'public-facing': 887, 'publication': 888, 'served': 889, 'server': 890, 'wider': 891, 'zenodo': 892, 'analysis': 893, 'best': 894, 'efforts': 895, 'environment': 896, 'lifecycle': 897, 'suite': 898, 'analyses': 899, 'cloud': 900, 'enables': 901, 'helps': 902, 'instantiating': 903, 'preserve': 904, 'preserved': 905, 'reana': 906, 'reusable': 907, 'abovementioned': 908, 'account': 909, 'appropriate': 910, 'carried': 911, 'commission': 912, 'compliance': 913, 'fair': 914, 'force': 915, 'out': 916, 'plan': 917, 'possible': 918, 'principles': 919, 's': 920, 'strive': 921, 'taking': 922, 'using': 923, 'cerns': 924, 'certain': 925, 'daily': 926, 'provides': 927, 'superconducting': 928, 'synchro-cyclotron': 929, 'tours': 930, 'workshop': 931, '-m': 932, 'dancing': 933, 'form': 934, 'god': 935, 'hindu': 936, 'nataraja': 937, 'shiva': 938, 'statue': 939, 'unveiled': 940, 'celebrate': 941, 'center': 942, 'cosmic': 943, 'dance': 944, 'destruction': 945, 'government': 946, 'india': 947, 'indian': 948, 'long': 949, 'presented': 950, 'symbolizing': 951, 'ago': 952, 'artists': 953, 'beautiful': 954, 'bronzes': 955, 'capra': 956, 'explains': 957, 'fritjof': 958, 'hundreds': 959, 'images': 960, 'metaphor': 961, 'physicist': 962, 'plaque': 963, 'quotations': 964, 'series': 965, 'shivas': 966, 'visual': 967, 'advanced': 968, 'our': 969, 'patterns': 970, 'portray': 971, 'ancient': 972, 'art': 973, 'modern': 974, 'mythology': 975, 'religious': 976, 'thus': 977, 'unifies': 978, '/': 979, 'coordinates': 980, '°': 981, '′': 982, '″e\\ufeff': 983, '″n': 984, '\\ufeff': 985, '°n': 986, '°e\\ufeff': 987}\n"
     ]
    }
   ],
   "source": [
    "#Printing cuspus of both the 1st documents\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "06c0ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "019e8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['(', 0.07], [')', 0.07], [',', 0.03], ['/sɜːrn/', 0.23], [':', 0.11], [';', 0.31], ['[', 0.15], [']', 0.15], ['an', 0.13], ['as', 0.08], ['cern', 0.04], ['conseil', 0.2], ['european', 0.14], ['européen', 0.18], ['for', 0.07], ['french', 0.14], ['in', 0.04], ['intergovernmental', 0.2], ['is', 0.08], ['known', 0.18], ['la', 0.16], ['laboratory', 0.14], ['largest', 0.18], ['nuclear', 0.18], ['nucléaire', 0.18], ['operates', 0.18], ['organization', 0.33], ['particle', 0.1], ['physics', 0.1], ['pour', 0.16], ['pronunciation', 0.23], ['recherche', 0.18], ['research', 0.1], ['sɛʁn', 0.23], ['that', 0.11], ['the', 0.03], ['world', 0.12], ['\\u200b', 0.23]]\n",
      "[[',', 0.06], ['in', 0.16], ['is', 0.14], ['the', 0.02], ['a', 0.1], ['based', 0.29], ['border', 0.29], ['established', 0.31], ['france–switzerland', 0.41], ['geneva', 0.31], ['it', 0.23], ['northwestern', 0.41], ['of', 0.05], ['on', 0.14], ['suburb', 0.41]]\n",
      "[['(', 0.1], [')', 0.1], [',', 0.02], ['in', 0.06], ['is', 0.11], ['the', 0.01], ['it', 0.18], ['admitted', 0.32], ['and', 0.05], ['comprises', 0.32], ['country', 0.32], ['currently', 0.2], ['full', 0.28], ['holding', 0.32], ['israel', 0.28], ['member', 0.21], ['membership', 0.28], ['non-european', 0.28], ['only', 0.28], ['states', 0.21]]\n",
      "[['an', 0.24], ['cern', 0.08], ['is', 0.15], ['assembly', 0.44], ['general', 0.37], ['nations', 0.44], ['observer', 0.34], ['official', 0.37], ['united', 0.37]]\n",
      "[[',', 0.11], [';', 0.18], ['cern', 0.05], ['in', 0.1], ['is', 0.09], ['laboratory', 0.17], ['the', 0.02], ['it', 0.15], ['and', 0.08], ['about', 0.27], ['acronym', 0.21], ['administrative', 0.27], ['also', 0.15], ['countries', 0.19], ['from', 0.14], ['had', 0.23], ['hosted', 0.27], ['institutions', 0.23], ['members', 0.18], ['more', 0.18], ['refer', 0.27], ['scientific', 0.17], ['staff', 0.23], ['technical', 0.27], ['than', 0.23], ['to', 0.1], ['used', 0.16], ['users', 0.27]]\n",
      "[[',', 0.05], ['cern', 0.12], ['in', 0.12], ['of', 0.08], ['data', 0.34], ['generated', 0.65], ['petabytes', 0.65]]\n",
      "[[',', 0.02], ['cern', 0.1], ['for', 0.08], ['is', 0.1], ['particle', 0.12], ['physics', 0.12], ['research', 0.13], ['the', 0.01], ['and', 0.04], ['to', 0.05], [\"'s\", 0.13], ['accelerators', 0.18], ['at', 0.11], ['been', 0.18], ['collaborations', 0.24], ['consequently', 0.28], ['constructed', 0.28], ['experiments', 0.12], ['function', 0.28], ['have', 0.16], ['high-energy', 0.2], ['infrastructure', 0.24], ['international', 0.22], ['main', 0.17], ['needed', 0.2], ['numerous', 0.28], ['other', 0.15], ['provide', 0.28], ['through', 0.18], ['—', 0.28]]\n",
      "[['(', 0.13], [')', 0.13], [',', 0.03], ['cern', 0.07], ['is', 0.14], ['largest', 0.31], ['particle', 0.17], ['the', 0.05], ['world', 0.21], ['of', 0.05], ['and', 0.06], [\"'s\", 0.19], ['collider', 0.54], ['hadron', 0.35], ['highest-energy', 0.41], ['large', 0.27], ['lhc', 0.15], ['site', 0.22]]\n",
      "[[',', 0.04], ['as', 0.21], ['is', 0.1], ['the', 0.01], ['a', 0.07], ['and', 0.05], ['from', 0.15], ['to', 0.05], ['used', 0.18], ['data', 0.15], ['at', 0.11], ['experiments', 0.13], ['main', 0.18], ['large', 0.2], ['site', 0.16], ['analyze', 0.3], ['computing', 0.2], ['events', 0.3], ['facility', 0.25], ['hosts', 0.25], ['meyrin', 0.21], ['primarily', 0.3], ['simulate', 0.3], ['store', 0.3], ['well', 0.25], ['which', 0.13]]\n",
      "[[',', 0.02], ['as', 0.11], ['the', 0.01], ['a', 0.07], ['to', 0.06], ['been', 0.19], ['access', 0.17], ['area', 0.21], ['facilities', 0.24], ['has', 0.17], ['historically', 0.31], ['hub', 0.31], ['lab', 0.31], ['major', 0.31], ['network', 0.27], ['remote', 0.31], ['require', 0.31], ['researchers', 0.22], ['these', 0.21], ['wide', 0.19]]\n",
      "[['cern', 0.11], ['is', 0.22], ['the', 0.05], ['world', 0.32], ['of', 0.08], ['also', 0.35], ['wide', 0.39], ['birthplace', 0.63], ['web', 0.39]]\n",
      "[['cern', 0.08], ['in', 0.08], ['the', 0.02], ['on', 0.15], ['countries', 0.31], ['by', 0.15], ['convention', 0.31], ['establishing', 0.38], ['europe', 0.38], ['ratified', 0.38], ['september', 0.34], ['was', 0.12], ['western', 0.44]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.03], ['cern', 0.05], ['conseil', 0.21], ['european', 0.16], ['européen', 0.19], ['for', 0.21], ['french', 0.16], ['in', 0.05], ['la', 0.18], ['laboratory', 0.16], ['nuclear', 0.19], ['nucléaire', 0.19], ['pour', 0.18], ['recherche', 0.19], ['research', 0.11], ['the', 0.03], ['a', 0.06], ['established', 0.19], ['acronym', 0.19], ['which', 0.11], ['by', 0.09], ['was', 0.07], [\"'\", 0.21], [\"'european\", 0.21], ['building', 0.25], ['council', 0.31], ['governments', 0.25], ['originally', 0.19], ['provisional', 0.19], ['represented', 0.19], ['words', 0.25]]\n",
      "[[',', 0.02], ['in', 0.05], ['the', 0.03], ['geneva', 0.21], ['of', 0.07], ['to', 0.05], ['at', 0.1], ['site', 0.15], ['these', 0.18], ['council', 0.17], ['before', 0.24], ['bohr', 0.24], ['copenhagen', 0.28], ['direction', 0.28], ['during', 0.24], ['early', 0.2], ['its', 0.16], ['moving', 0.28], ['niels', 0.28], ['present', 0.28], ['under', 0.21], ['university', 0.28], ['worked', 0.24], ['years', 0.21]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.02], ['for', 0.14], ['in', 0.05], ['la', 0.18], ['laboratory', 0.15], ['nuclear', 0.19], ['nucléaire', 0.19], ['organization', 0.18], ['pour', 0.18], ['recherche', 0.19], ['research', 0.11], ['the', 0.05], ['acronym', 0.19], ['to', 0.05], ['was', 0.14], [\"'\", 0.21], [\"'european\", 0.21], ['council', 0.15], ['provisional', 0.19], ['after', 0.15], ['changed', 0.21], ['current', 0.21], ['dissolved', 0.25], ['européenne', 0.25], ['even', 0.21], ['name', 0.21], ['new', 0.15], ['organisation', 0.21], ['retained', 0.25], ['though', 0.25]]\n",
      "[[',', 0.05], ['[', 0.13], [']', 0.13], ['cern', 0.07], ['is', 0.07], ['that', 0.09], ['the', 0.03], ['a', 0.05], ['of', 0.02], ['and', 0.03], ['to', 0.04], ['have', 0.11], ['was', 0.05], ['changed', 0.17], ['even', 0.17], ['name', 0.33], [\"''\", 0.14], ['``', 0.14], ['abbreviation', 0.19], ['according', 0.17], ['awkward', 0.19], ['be', 0.11], ['become', 0.17], ['could', 0.39], ['director', 0.17], ['former', 0.19], ['heisenberg', 0.19], ['if', 0.19], ['kowarski', 0.19], ['lew', 0.19], ['not', 0.19], ['oern', 0.19], ['said', 0.19], ['still', 0.15], ['this', 0.12], ['werner', 0.19], ['when', 0.13]]\n",
      "[['cern', 0.09], [\"'s\", 0.22], ['was', 0.13], ['benjamin', 0.47], ['first', 0.23], ['lockspeiser', 0.47], ['president', 0.47], ['sir', 0.47]]\n",
      "[['(', 0.09], [')', 0.09], [',', 0.02], ['cern', 0.05], ['the', 0.02], ['of', 0.03], ['general', 0.24], ['at', 0.1], ['was', 0.15], ['provisional', 0.21], ['early', 0.2], ['its', 0.16], ['still', 0.21], ['when', 0.18], ['first', 0.14], ['amaldi', 0.28], ['bloch', 0.28], ['director-general', 0.28], ['edoardo', 0.28], ['felix', 0.28], ['operations', 0.28], ['secretary', 0.28], ['stages', 0.28], ['were', 0.16], ['while', 0.21]]\n",
      "[[',', 0.03], ['laboratory', 0.15], ['physics', 0.1], ['the', 0.03], ['of', 0.06], ['to', 0.09], ['was', 0.13], ['originally', 0.19], ['applied', 0.24], ['atomic', 0.24], ['between', 0.17], ['but', 0.17], ['concerned', 0.24], ['devoted', 0.24], ['higher-energy', 0.24], ['interactions', 0.21], ['mainly', 0.24], ['nuclei', 0.24], ['particles', 0.21], ['soon', 0.21], ['study', 0.48], ['subatomic', 0.24], ['with', 0.11]]\n",
      "[['(', 0.07], [')', 0.07], [',', 0.03], ['as', 0.09], ['cern', 0.04], ['european', 0.15], ['européen', 0.18], ['for', 0.07], ['is', 0.09], ['la', 0.17], ['laboratory', 0.3], ['particle', 0.1], ['physics', 0.1], ['pour', 0.17], ['research', 0.11], ['the', 0.03], ['to', 0.04], ['which', 0.1], ['by', 0.08], ['being', 0.24], ['better', 0.24], ['commonly', 0.24], ['des', 0.24], ['describes', 0.24], ['laboratoire', 0.24], ['operated', 0.24], ['particules', 0.24], ['performed', 0.18], ['physique', 0.24], ['referred', 0.24], ['there', 0.21], ['therefore', 0.24]]\n",
      "[[',', 0.07], ['[', 0.16], [']', 0.16], ['cern', 0.04], ['in', 0.05], ['organization', 0.17], ['the', 0.04], ['of', 0.03], ['states', 0.16], ['from', 0.13], ['to', 0.04], ['at', 0.09], ['needed', 0.17], ['which', 0.11], ['by', 0.08], ['convention', 0.17], ['establishing', 0.21], ['was', 0.07], ['council', 0.15], ['citation', 0.24], ['july', 0.21], ['june', 0.24], ['paris', 0.24], ['place', 0.24], ['ratification', 0.24], ['session', 0.24], ['signed', 0.24], ['sixth', 0.24], ['subject', 0.24], ['took', 0.24], ['–', 0.24]]\n",
      "[[',', 0.18], [':', 0.11], ['the', 0.05], ['of', 0.03], ['and', 0.04], ['member', 0.16], ['states', 0.16], ['united', 0.2], ['by', 0.08], ['convention', 0.17], ['ratified', 0.2], ['was', 0.07], ['belgium', 0.2], ['denmark', 0.24], ['federal', 0.24], ['founding', 0.18], ['france', 0.2], ['germany', 0.2], ['gradually', 0.24], ['greece', 0.24], ['italy', 0.24], ['kingdom', 0.24], ['netherlands', 0.24], ['norway', 0.24], ['republic', 0.2], ['sweden', 0.24], ['switzerland', 0.17], ['yugoslavia', 0.17]]\n",
      "[['cern', 0.08], ['in', 0.08], ['particle', 0.18], ['physics', 0.18], ['at', 0.16], ['been', 0.26], ['experiments', 0.18], ['have', 0.24], ['through', 0.26], ['achievements', 0.42], ['important', 0.42], ['made', 0.36], ['several', 0.42]]\n",
      "[[',', 0.02], [':', 0.13], ['cern', 0.05], ['in', 0.06], ['the', 0.02], ['of', 0.04], ['september', 0.22], ['when', 0.19], ['attention', 0.29], ['attracted', 0.29], ['collaboration', 0.2], ['detection', 0.29], ['faster-than-light', 0.29], ['include', 0.25], ['media', 0.29], ['neutrinos', 0.29], ['opera', 0.29], ['possibly', 0.29], ['reported', 0.29], ['they', 0.2]]\n",
      "[['an', 0.16], ['that', 0.14], ['the', 0.01], ['to', 0.05], ['were', 0.17], ['cable', 0.3], ['connected', 0.3], ['due', 0.3], ['flawed', 0.3], ['further', 0.26], ['gps', 0.3], ['incorrectly', 0.3], ['results', 0.23], ['showed', 0.3], ['synchronization', 0.3], ['tests', 0.3]]\n",
      "[['for', 0.15], ['in', 0.05], ['physics', 0.11], ['that', 0.13], ['the', 0.04], ['of', 0.03], ['and', 0.08], ['to', 0.05], ['was', 0.07], ['awarded', 0.2], ['bosons', 0.26], ['carlo', 0.26], ['der', 0.26], ['developments', 0.26], ['discoveries', 0.26], ['meer', 0.26], ['nobel', 0.2], ['prize', 0.2], ['resulted', 0.26], ['rubbia', 0.26], ['simon', 0.26], ['van', 0.26], ['w', 0.26], ['z', 0.26]]\n",
      "[[',', 0.02], ['cern', 0.05], ['for', 0.15], ['in', 0.05], ['particle', 0.11], ['physics', 0.11], ['the', 0.02], ['of', 0.03], ['and', 0.04], ['staff', 0.22], ['to', 0.05], ['was', 0.07], [\"''\", 0.18], ['``', 0.18], ['awarded', 0.2], ['nobel', 0.2], ['prize', 0.2], ['chamber', 0.22], ['charpak', 0.26], ['detectors', 0.26], ['development', 0.2], ['georges', 0.26], ['his', 0.26], ['invention', 0.26], ['multiwire', 0.26], ['particular', 0.26], ['proportional', 0.26], ['researcher', 0.26]]\n",
      "[['cern', 0.05], ['for', 0.14], ['in', 0.05], ['physics', 0.1], ['the', 0.05], ['of', 0.03], ['and', 0.04], ['to', 0.05], ['experiments', 0.11], ['by', 0.09], ['was', 0.14], ['after', 0.16], ['awarded', 0.19], ['nobel', 0.19], ['prize', 0.19], ['boson', 0.18], ['description', 0.25], ['englert', 0.25], ['found', 0.21], ['françois', 0.25], ['higgs', 0.53], ['mechanism', 0.25], ['peter', 0.25], ['theoretical', 0.25], ['year', 0.25]]\n",
      "[[',', 0.02], ['as', 0.12], ['cern', 0.06], ['in', 0.13], ['the', 0.01], ['world', 0.18], ['a', 0.08], ['and', 0.05], ['wide', 0.22], ['web', 0.22], ['by', 0.12], ['began', 0.3], ['berners-lee', 0.27], ['cailliau', 0.3], ['enquire', 0.35], ['initiated', 0.3], ['named', 0.25], ['project', 0.2], ['robert', 0.35], ['tim', 0.35]]\n",
      "[['for', 0.19], ['in', 0.06], ['the', 0.04], ['world', 0.17], ['of', 0.04], ['and', 0.05], ['to', 0.06], ['computing', 0.22], ['wide', 0.21], ['web', 0.21], ['by', 0.11], ['were', 0.19], ['development', 0.26], ['berners-lee', 0.26], ['cailliau', 0.28], ['association', 0.28], ['contributions', 0.28], ['honoured', 0.33], ['jointly', 0.33], ['machinery', 0.33], ['their', 0.2]]\n",
      "[[',', 0.02], ['the', 0.04], ['based', 0.25], ['of', 0.09], ['on', 0.12], ['to', 0.07], ['researchers', 0.25], ['was', 0.1], ['between', 0.25], ['project', 0.2], ['concept', 0.3], ['facilitate', 0.36], ['hypertext', 0.36], ['information', 0.36], ['intended', 0.36], ['sharing', 0.36]]\n",
      "[['in', 0.13], ['the', 0.03], ['was', 0.19], ['first', 0.34], ['activated', 0.69], ['website', 0.59]]\n",
      "[[',', 0.03], ['cern', 0.08], ['that', 0.2], ['the', 0.02], ['world', 0.21], ['on', 0.14], ['to', 0.08], ['wide', 0.26], ['web', 0.26], ['be', 0.23], ['announced', 0.32], ['anyone', 0.41], ['april', 0.29], ['free', 0.41], ['would', 0.41]]\n",
      "[[',', 0.04], ['as', 0.11], ['is', 0.11], ['the', 0.02], ['world', 0.16], ['a', 0.15], ['of', 0.04], ['on', 0.1], [\"'s\", 0.14], ['wide', 0.19], ['web', 0.19], ['by', 0.11], ['still', 0.24], ['first', 0.15], ['berners-lee', 0.24], ['website', 0.26], ['consortium', 0.26], ['copy', 0.31], ['created', 0.26], ['document', 0.22], ['historical', 0.26], ['original', 0.31], ['published', 0.24], ['webpage', 0.31]]\n",
      "[['(', 0.06], [')', 0.06], [',', 0.05], ['cern', 0.07], ['for', 0.11], ['in', 0.04], ['the', 0.04], ['a', 0.05], ['of', 0.05], ['and', 0.03], ['had', 0.17], ['more', 0.13], ['to', 0.04], [\"'s\", 0.09], ['lhc', 0.07], ['computing', 0.26], ['facility', 0.17], ['has', 0.11], ['web', 0.12], ['early', 0.14], ['become', 0.17], ['development', 0.3], ['beginning', 0.2], ['e-science', 0.2], ['egee', 0.2], ['enabling', 0.2], ['grid', 0.34], ['grids', 0.2], ['hosting', 0.2], ['including', 0.2], ['internet', 0.17], ['introduction', 0.2], ['pioneered', 0.2], ['prior', 0.17], ['projects', 0.2], ['recently', 0.17], ['s.', 0.2], ['technology', 0.15]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.02], ['cern', 0.05], ['in', 0.05], ['the', 0.02], ['it', 0.15], ['of', 0.03], ['also', 0.15], ['main', 0.16], ['hosts', 0.23], ['switzerland', 0.19], ['internet', 0.47], ['cixp', 0.27], ['exchange', 0.54], ['one', 0.27], ['point', 0.27], ['points', 0.23], ['two', 0.15]]\n",
      "[['[', 0.22], [']', 0.22], ['as', 0.12], ['cern', 0.06], ['research', 0.15], ['of', 0.04], ['and', 0.05], ['more', 0.22], ['than', 0.29], ['employs', 0.34], ['engineers', 0.29], ['physicists', 0.26], ['technicians', 0.34], ['ten', 0.34], ['times', 0.34], ['update', 0.34]]\n",
      "[[',', 0.03], ['cern', 0.07], ['operates', 0.28], ['a', 0.09], ['of', 0.05], ['and', 0.11], ['accelerators', 0.45], ['network', 0.31], ['two', 0.2], ['additional', 0.36], ['decelerators', 0.31], ['seven', 0.31], ['small', 0.36], ['some', 0.31]]\n",
      "[['(', 0.06], [')', 0.06], ['in', 0.04], ['particle', 0.16], ['the', 0.04], ['of', 0.05], ['more', 0.13], ['to', 0.11], ['experiments', 0.17], ['before', 0.33], ['further', 0.16], ['decelerators', 0.16], ['accelerator', 0.1], ['accelerators/decelerators', 0.19], ['beams', 0.26], ['chain', 0.19], ['decrease', 0.19], ['delivering', 0.38], ['each', 0.11], ['energy', 0.24], ['increases', 0.19], ['machine', 0.19], ['naturally', 0.19], ['next', 0.16], ['or', 0.3], ['powerful', 0.19], ['them', 0.27]]\n",
      "[['(', 0.18], [')', 0.18], [':', 0.13], ['as', 0.1], ['cern', 0.05], ['for', 0.08], ['the', 0.03], ['it', 0.16], ['of', 0.04], ['and', 0.09], ['currently', 0.36], ['at', 0.11], ['experiments', 0.13], ['collider', 0.19], ['hadron', 0.25], ['large', 0.19], ['lhc', 0.21], ['accelerator', 0.16], ['active', 0.29], ['activities', 0.25], ['are', 0.11], ['involve', 0.29], ['machines', 0.29], ['many', 0.29], ['operating', 0.29]]\n",
      "[[',', 0.03], ['the', 0.02], ['a', 0.11], ['scientific', 0.3], ['lhc', 0.17], ['project', 0.27], ['cooperation', 0.48], ['large-scale', 0.48], ['represents', 0.41], ['worldwide', 0.41]]\n",
      "[[',', 0.02], ['in', 0.06], ['is', 0.11], ['the', 0.05], ['geneva', 0.25], ['and', 0.05], ['international', 0.25], ['lhc', 0.12], ['between', 0.23], ['airport', 0.32], ['jura', 0.32], ['located', 0.21], ['metres', 0.32], ['mountains', 0.32], ['nearby', 0.32], ['region', 0.32], ['tunnel', 0.27], ['underground', 0.23]]\n",
      "[['french', 0.31], ['is', 0.17], ['the', 0.06], ['border', 0.35], ['of', 0.12], ['on', 0.17], ['its', 0.28], ['length', 0.49], ['majority', 0.49], ['side', 0.38]]\n",
      "[['(', 0.09], [')', 0.09], [',', 0.02], ['in', 0.05], ['the', 0.02], ['it', 0.16], ['collider', 0.19], ['large', 0.19], ['which', 0.12], ['by', 0.1], ['was', 0.08], ['tunnel', 0.24], ['circular', 0.24], ['circumference', 0.28], ['down', 0.24], ['electron–positron', 0.28], ['km', 0.28], ['lep', 0.22], ['november', 0.24], ['occupied', 0.28], ['previously', 0.28], ['shut', 0.28], ['uses', 0.28]]\n",
      "[['cern', 0.06], ['the', 0.01], ['and', 0.05], ['to', 0.06], ['used', 0.18], [\"'s\", 0.14], ['lhc', 0.11], ['which', 0.13], ['accelerator', 0.17], ['are', 0.22], ['complexes', 0.31], ['existing', 0.31], ['injected', 0.26], ['into', 0.21], ['ions', 0.31], ['lead', 0.31], ['pre-accelerate', 0.31], ['protons', 0.31], ['ps/sps', 0.31], ['then', 0.22]]\n",
      "[['(', 0.07], [')', 0.07], [',', 0.11], [';', 0.15], ['particle', 0.1], ['the', 0.01], ['a', 0.06], ['of', 0.03], ['and', 0.07], ['from', 0.12], ['experiments', 0.1], ['collider', 0.15], ['with', 0.1], ['each', 0.14], ['them', 0.16], ['are', 0.08], ['located', 0.15], ['alice', 0.2], ['along', 0.23], ['aspect', 0.23], ['atlas', 0.18], ['cms', 0.18], ['collisions', 0.2], ['different', 0.39], ['eight', 0.23], ['faser', 0.23], ['lhcb', 0.2], ['lhcf', 0.23], ['moedal', 0.23], ['studies', 0.23], ['technologies', 0.23], ['totem', 0.23]]\n",
      "[['an', 0.23], ['for', 0.12], ['experiments', 0.19], ['these', 0.29], ['construction', 0.33], ['effort', 0.37], ['engineering', 0.43], ['extraordinary', 0.43], ['required', 0.43]]\n",
      "[[',', 0.05], ['for', 0.07], ['the', 0.01], ['a', 0.06], ['of', 0.03], ['from', 0.14], ['to', 0.05], ['was', 0.07], ['its', 0.15], ['belgium', 0.22], ['each', 0.15], ['into', 0.17], ['cms', 0.2], ['cavern', 0.26], ['crane', 0.26], ['detector', 0.26], ['example', 0.22], ['lower', 0.26], ['nearly', 0.26], ['piece', 0.26], ['pieces', 0.26], ['rented', 0.26], ['since', 0.16], ['special', 0.2], ['tons', 0.26], ['weighed', 0.26]]\n",
      "[[',', 0.02], [':', 0.16], ['for', 0.1], ['the', 0.03], ['a', 0.08], ['of', 0.04], ['on', 0.11], ['at', 0.13], ['was', 0.09], ['first', 0.17], ['down', 0.29], ['construction', 0.26], ['special', 0.26], ['approximately', 0.34], ['gmt', 0.34], ['lowered', 0.34], ['magnets', 0.29], ['march', 0.24], ['necessary', 0.29], ['shaft', 0.34]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.03], ['cern', 0.04], ['for', 0.07], ['the', 0.03], ['world', 0.13], ['a', 0.06], ['of', 0.06], ['to', 0.09], ['data', 0.13], ['infrastructure', 0.21], ['lhc', 0.18], ['computing', 0.16], ['which', 0.11], ['has', 0.13], ['grid', 0.41], ['around', 0.19], ['begun', 0.21], ['distributed', 0.24], ['generate', 0.24], ['laboratories', 0.24], ['making', 0.21], ['processing', 0.24], ['quantities', 0.24], ['specialized', 0.24], ['streams', 0.24], ['use', 0.21], ['vast', 0.24]]\n",
      "[[',', 0.03], ['the', 0.01], ['world', 0.19], ['a', 0.09], ['to', 0.07], ['during', 0.31], ['april', 0.26], ['seven', 0.31], ['different', 0.31], ['across', 0.28], ['mb/s', 0.36], ['sites', 0.21], ['streamed', 0.36], ['successfully', 0.28], ['trial', 0.36]]\n",
      "[['particle', 0.2], ['the', 0.04], ['lhc', 0.18], ['were', 0.27], ['beams', 0.32], ['injected', 0.41], ['into', 0.32], ['august', 0.49], ['initial', 0.49]]\n",
      "[[',', 0.03], ['for', 0.07], ['the', 0.03], ['a', 0.06], ['it', 0.14], ['of', 0.03], ['on', 0.16], ['and', 0.04], ['through', 0.15], ['lhc', 0.09], ['september', 0.37], ['was', 0.13], ['first', 0.12], ['but', 0.17], ['beam', 0.24], ['because', 0.24], ['circulated', 0.24], ['connection', 0.24], ['days', 0.24], ['entire', 0.21], ['failed', 0.24], ['faulty', 0.24], ['later', 0.21], ['magnet', 0.21], ['repairs', 0.24], ['stopped', 0.24], ['system', 0.24]]\n",
      "[[',', 0.03], ['an', 0.21], ['the', 0.02], ['of', 0.05], ['on', 0.13], ['lhc', 0.14], ['by', 0.13], ['with', 0.17], ['two', 0.22], ['beams', 0.26], ['each', 0.23], ['energy', 0.24], ['november', 0.33], ['successfully', 0.3], ['circulating', 0.39], ['operation', 0.39], ['resumed', 0.39]]\n",
      "[['(', 0.24], [')', 0.24], ['teraelectronvolts', 0.77], ['tev', 0.55]]\n",
      "[['for', 0.09], ['that', 0.15], ['the', 0.04], ['to', 0.12], ['other', 0.17], ['was', 0.09], ['they', 0.23], ['two', 0.18], ['engineers', 0.27], ['beams', 0.21], ['each', 0.19], ['into', 0.21], ['then', 0.23], ['challenge', 0.32], ['line', 0.27], ['smashed', 0.32], ['so', 0.32], ['try', 0.32], ['up', 0.32]]\n",
      "[[',', 0.02], ['for', 0.07], ['is', 0.09], ['the', 0.01], ['and', 0.08], ['to', 0.1], ['accelerators', 0.17], ['other', 0.14], [\"''\", 0.19], ['``', 0.19], ['according', 0.23], ['director', 0.23], ['this', 0.17], ['technology', 0.2], ['two', 0.15], ['each', 0.16], ['them', 0.19], ['across', 0.2], ['atlantic', 0.26], ['firing', 0.26], ['getting', 0.26], ['hit', 0.26], ['like', 0.26], ['myers', 0.26], ['needles', 0.26], ['steve', 0.26]]\n",
      "[[',', 0.04], ['the', 0.02], ['on', 0.17], ['lhc', 0.18], ['with', 0.23], ['two', 0.29], ['beams', 0.34], ['march', 0.36], ['successfully', 0.39], ['collided', 0.51], ['proton', 0.39]]\n",
      "[[',', 0.03], ['in', 0.07], ['a', 0.09], ['of', 0.05], ['energy', 0.48], ['tev', 0.55], ['proton', 0.3], ['collision', 0.33], ['per', 0.33], ['resulting', 0.38]]\n",
      "[[',', 0.02], ['for', 0.1], ['the', 0.04], ['of', 0.09], ['needed', 0.25], ['was', 0.2], ['this', 0.22], ['boson', 0.25], ['higgs', 0.25], ['discovery', 0.31], ['expected', 0.36], ['however', 0.31], ['just', 0.36], ['start', 0.36], ['what', 0.36]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.04], ['particle', 0.11], ['that', 0.13], ['the', 0.02], ['and', 0.04], ['to', 0.05], ['at', 0.1], ['lhc', 0.1], ['when', 0.18], ['soon', 0.23], ['began', 0.23], ['energy', 0.17], ['collisions', 0.23], ['march', 0.19], ['tev', 0.57], ['proton', 0.2], ['per', 0.23], ['ended', 0.27], ['experimental', 0.2], ['period', 0.19], ['revved', 0.27], ['starting', 0.23]]\n",
      "[[',', 0.02], ['cern', 0.06], ['in', 0.07], ['particle', 0.15], ['that', 0.17], ['the', 0.03], ['a', 0.08], ['of', 0.04], ['to', 0.06], ['was', 0.1], ['new', 0.22], ['be', 0.2], ['july', 0.3], ['boson', 0.25], ['higgs', 0.25], ['announced', 0.27], ['later', 0.3], ['discovery', 0.3], ['confirmed', 0.35], ['scientists', 0.35], ['sub-atomic', 0.35]]\n",
      "[[',', 0.02], ['cern', 0.06], ['in', 0.06], ['is', 0.11], ['particle', 0.14], ['that', 0.31], ['the', 0.03], ['a', 0.08], ['it', 0.18], ['on', 0.11], ['to', 0.06], ['this', 0.2], ['performed', 0.25], ['boson', 0.23], ['found', 0.28], ['higgs', 0.23], ['announced', 0.25], ['march', 0.23], ['allowed', 0.32], ['conclude', 0.32], ['measurements', 0.32], ['newly', 0.32]]\n",
      "[[',', 0.04], ['for', 0.17], ['in', 0.06], ['the', 0.04], ['a', 0.07], ['and', 0.05], ['to', 0.06], ['other', 0.16], ['lhc', 0.11], ['was', 0.08], ['early', 0.22], ['between', 0.22], ['accelerator', 0.17], ['magnets', 0.26], ['period', 0.22], ['connections', 0.31], ['deactivated', 0.31], ['electrical', 0.31], ['inside', 0.31], ['maintenance', 0.26], ['strengthen', 0.31], ['two-year', 0.26], ['upgrades', 0.26]]\n",
      "[[',', 0.05], ['for', 0.11], ['the', 0.02], ['a', 0.09], ['of', 0.05], ['on', 0.13], ['and', 0.06], ['lhc', 0.14], ['years', 0.3], ['after', 0.24], ['april', 0.27], ['two', 0.22], ['maintenance', 0.33], ['consolidation', 0.39], ['restarted', 0.39], ['run', 0.39], ['second', 0.33]]\n",
      "[['the', 0.05], ['of', 0.08], ['to', 0.11], ['first', 0.3], ['energy', 0.38], ['ramp', 0.61], ['record-breaking', 0.61]]\n",
      "[['on', 0.25], ['was', 0.21], ['performed', 0.58], ['april', 0.53], ['tev', 0.53]]\n",
      "[[',', 0.03], ['for', 0.13], ['in', 0.09], ['the', 0.04], ['was', 0.13], ['first', 0.23], ['collision', 0.39], ['design', 0.46], ['exceeded', 0.46], ['rate', 0.46], ['time', 0.35]]\n",
      "[['the', 0.02], ['a', 0.11], ['of', 0.11], ['at', 0.17], ['begun', 0.38], ['period', 0.32], ['two-year', 0.38], ['second', 0.38], ['end', 0.45], ['shutdown', 0.45]]\n",
      "[['(', 0.1], [')', 0.1], [',', 0.02], ['as', 0.11], ['in', 0.06], ['is', 0.11], ['the', 0.02], ['a', 0.07], ['of', 0.04], ['to', 0.06], [\"'s\", 0.14], ['lhc', 0.22], ['project', 0.17], ['construction', 0.24], ['called', 0.31], ['high', 0.31], ['hl-lhc', 0.26], ['luminosity', 0.52], ['october', 0.31], ['on-going', 0.31], ['upgrade', 0.26]]\n",
      "[['an', 0.19], ['the', 0.01], ['of', 0.04], ['to', 0.06], ['lhc', 0.13], ['by', 0.12], ['this', 0.22], ['project', 0.2], ['accelerator', 0.19], ['luminosity', 0.3], ['higher', 0.35], ['magnitude', 0.35], ['order', 0.35], ['see', 0.35], ['should', 0.3], ['upgraded', 0.35]]\n",
      "[[',', 0.03], ['as', 0.13], ['cern', 0.07], ['the', 0.01], ['of', 0.05], ['and', 0.06], ['also', 0.21], ['accelerators', 0.23], ['other', 0.19], ['project', 0.21], ['their', 0.22], ['are', 0.14], ['upgrades', 0.32], ['hl-lhc', 0.32], ['upgrade', 0.32], ['part', 0.37], ['receiving', 0.37], ['subsystems', 0.37]]\n",
      "[[',', 0.04], ['the', 0.02], ['a', 0.06], ['and', 0.04], ['other', 0.14], ['by', 0.09], ['was', 0.07], ['new', 0.16], ['accelerator', 0.28], ['among', 0.26], ['decommissioned', 0.26], ['injector', 0.52], ['linac', 0.52], ['linear', 0.22], ['replaced', 0.26], ['work', 0.22]]\n",
      "[['(', 0.07], [')', 0.07], [',', 0.04], [':', 0.1], ['cern', 0.04], ['for', 0.06], ['in', 0.04], ['is', 0.07], ['the', 0.02], ['a', 0.2], ['of', 0.03], ['and', 0.03], ['currently', 0.13], ['to', 0.04], ['accelerators', 0.13], ['main', 0.12], ['collider', 0.28], ['lhc', 0.08], ['new', 0.13], ['with', 0.19], ['collaboration', 0.15], ['named', 0.15], ['project', 0.12], ['concept', 0.18], ['two', 0.12], ['energy', 0.13], ['worldwide', 0.18], ['circular', 0.18], ['linear', 0.18], ['acceleration', 0.21], ['clic', 0.21], ['concepts', 0.21], ['electron-positron', 0.21], ['future', 0.36], ['groups', 0.21], ['increase', 0.21], ['investigating', 0.21], ['larger', 0.21], ['version', 0.21]]\n",
      "[['(', 0.08], [')', 0.08], [',', 0.04], ['as', 0.09], ['french', 0.16], ['in', 0.05], ['known', 0.2], ['the', 0.05], ['border', 0.37], ['on', 0.09], ['also', 0.15], ['to', 0.05], ['accelerators', 0.16], ['been', 0.16], ['main', 0.16], ['site', 0.14], ['meyrin', 0.19], ['which', 0.11], ['area', 0.17], ['has', 0.14], ['was', 0.07], ['originally', 0.2], ['but', 0.19], ['switzerland', 0.19], ['are', 0.1], ['since', 0.16], ['alongside', 0.26], ['built', 0.23], ['extended', 0.26], ['smaller', 0.26], ['span', 0.26], ['west', 0.23]]\n",
      "[[',', 0.02], ['french', 0.18], ['is', 0.2], ['the', 0.02], ['a', 0.07], ['border', 0.2], ['of', 0.04], ['and', 0.04], ['from', 0.15], ['site', 0.15], ['under', 0.22], ['there', 0.24], ['side', 0.22], ['line', 0.24], ['apart', 0.28], ['jurisdiction', 0.28], ['marker', 0.28], ['no', 0.28], ['obvious', 0.28], ['stones', 0.28], ['swiss', 0.28], ['within', 0.24]]\n",
      "[[',', 0.02], ['french', 0.18], ['the', 0.03], ['and', 0.13], ['from', 0.15], ['main', 0.17], ['site', 0.15], ['under', 0.22], ['are', 0.21], ['almost', 0.28], ['buried', 0.28], ['entirely', 0.28], ['farmland', 0.28], ['invisible', 0.28], ['lep/lhc', 0.28], ['mostly', 0.28], ['outside', 0.24], ['sps', 0.22], ['surface', 0.24], ['tunnels', 0.24]]\n",
      "[[',', 0.03], ['as', 0.17], ['the', 0.02], ['of', 0.03], ['and', 0.04], ['to', 0.04], ['at', 0.09], ['experiments', 0.1], ['have', 0.14], ['needed', 0.17], ['other', 0.12], ['access', 0.13], ['facilities', 0.19], ['with', 0.11], ['they', 0.17], ['points', 0.21], ['or', 0.19], ['them', 0.17], ['around', 0.19], ['sites', 0.14], ['however', 0.21], ['surface', 0.21], ['associated', 0.21], ['buildings', 0.24], ['colliders', 0.24], ['cryogenic', 0.21], ['either', 0.24], ['location', 0.24], ['operate', 0.24], ['plants', 0.24], ['shafts', 0.24], ['such', 0.15], ['various', 0.19]]\n",
      "[['as', 0.16], ['the', 0.05], ['at', 0.34], ['experiments', 0.2], ['these', 0.3], ['are', 0.16], ['located', 0.3], ['underground', 0.32], ['sites', 0.25], ['tunnels', 0.39], ['level', 0.39], ['same', 0.39]]\n",
      "[[',', 0.04], ['in', 0.17], ['the', 0.01], ['of', 0.07], ['and', 0.05], ['access', 0.16], ['these', 0.2], ['with', 0.13], ['france', 0.25], ['switzerland', 0.42], ['some', 0.25], ['are', 0.22], ['atlas', 0.23], ['sites', 0.34], ['experimental', 0.23], ['cryogenic', 0.25], ['although', 0.3], ['ancillary', 0.3], ['three', 0.25]]\n",
      "[[',', 0.04], ['as', 0.11], ['for', 0.09], ['is', 0.23], ['known', 0.25], ['largest', 0.25], ['the', 0.08], ['of', 0.04], ['on', 0.11], ['also', 0.18], ['experiments', 0.14], ['site', 0.17], ['which', 0.14], ['area', 0.21], ['accelerator', 0.17], ['sites', 0.18], ['experimental', 0.25], ['sps', 0.25], ['non-collider', 0.32], ['north', 0.28], ['prévessin', 0.25], ['station', 0.32], ['target', 0.32]]\n",
      "[['(', 0.1], [')', 0.1], [',', 0.02], ['for', 0.09], ['the', 0.05], ['and', 0.05], ['used', 0.38], ['experiments', 0.28], ['other', 0.17], ['lhc', 0.12], ['which', 0.14], ['by', 0.11], ['were', 0.18], ['are', 0.23], ['lep', 0.24], ['sites', 0.18], ['latter', 0.32], ['ones', 0.32], ['ua', 0.54]]\n",
      "[[',', 0.03], ['the', 0.03], ['of', 0.05], ['and', 0.11], ['experiments', 0.16], ['lhc', 0.13], ['site', 0.2], ['after', 0.23], ['were', 0.21], ['they', 0.26], ['named', 0.26], ['are', 0.13], ['located', 0.24], ['lep', 0.28], ['outside', 0.31], ['most', 0.26], ['numbered', 0.36], ['officially', 0.36], ['where', 0.31]]\n",
      "[['(', 0.18], [')', 0.18], [',', 0.01], ['an', 0.1], ['european', 0.12], ['for', 0.05], ['the', 0.03], ['of', 0.02], ['and', 0.03], ['to', 0.04], ['used', 0.11], ['at', 0.22], ['site', 0.21], ['meyrin', 0.14], ['area', 0.26], ['was', 0.05], [\"''\", 0.14], ['``', 0.14], ['while', 0.15], ['interactions', 0.17], ['particles', 0.17], ['chamber', 0.17], ['located', 0.13], ['example', 0.17], ['west', 0.17], ['north', 0.17], ['prévessin', 0.15], ['bebc', 0.19], ['big', 0.19], ['bubble', 0.19], ['charmed', 0.19], ['examine', 0.19], ['experiment', 0.19], ['looking', 0.19], ['na', 0.19], ['neutrino', 0.19], ['production', 0.19], ['so-called', 0.19], ['wa', 0.19]]\n",
      "[[',', 0.03], ['in', 0.07], ['the', 0.03], ['and', 0.06], ['to', 0.07], ['experiments', 0.17], ['area', 0.25], ['be', 0.22], ['were', 0.22], ['underground', 0.27], ['ua', 0.66], ['considered', 0.38], ['i.e', 0.38]]\n",
      "[['the', 0.02], ['on', 0.2], ['at', 0.22], ['accelerator', 0.31], ['underground', 0.41], ['sites', 0.33], ['sps', 0.45], ['situated', 0.58]]\n",
      "[[',', 0.04], ['as', 0.11], ['cern', 0.11], ['for', 0.08], ['the', 0.02], ['of', 0.04], ['on', 0.1], ['and', 0.05], [\"'s\", 0.14], ['meyrin', 0.21], ['after', 0.19], ['named', 0.21], ['physicists', 0.23], ['are', 0.11], ['sites', 0.17], ['such', 0.19], ['prévessin', 0.23], ['most', 0.21], ['creation', 0.26], ['famous', 0.3], ['pauli', 0.3], ['pushed', 0.3], ['roads', 0.3], ['who', 0.26], ['wolfgang', 0.3]]\n",
      "[[',', 0.05], ['and', 0.06], ['other', 0.19], ['bohr', 0.32], ['are', 0.14], ['albert', 0.37], ['einstein', 0.37], ['feynman', 0.37], ['names', 0.37], ['notable', 0.37], ['richard', 0.37]]\n",
      "[[',', 0.03], ['cern', 0.07], ['in', 0.08], ['members', 0.54], ['by', 0.14], ['its', 0.23], ['new', 0.25], ['since', 0.25], ['accepted', 0.41], ['foundation', 0.41], ['regularly', 0.41]]\n",
      "[[',', 0.02], ['in', 0.07], ['organization', 0.25], ['the', 0.01], ['and', 0.05], ['members', 0.23], ['have', 0.2], ['new', 0.22], ['yugoslavia', 0.25], ['their', 0.21], ['since', 0.22], ['accession', 0.35], ['all', 0.25], ['continuously', 0.35], ['except', 0.35], ['remained', 0.35], ['spain', 0.27]]\n",
      "[[',', 0.07], ['cern', 0.09], ['in', 0.29], ['and', 0.08], ['first', 0.25], ['spain', 0.39], ['joined', 0.43], ['rejoined', 0.5], ['withdrew', 0.5]]\n",
      "[['cern', 0.1], ['in', 0.11], ['a', 0.13], ['of', 0.07], ['member', 0.37], ['was', 0.15], ['but', 0.39], ['founding', 0.43], ['yugoslavia', 0.39], ['quit', 0.55]]\n",
      "[['(', 0.09], [')', 0.09], [',', 0.04], ['as', 0.11], ['cern', 0.05], ['the', 0.02], ['a', 0.07], ['of', 0.04], ['on', 0.1], ['and', 0.05], ['currently', 0.19], ['full', 0.51], ['israel', 0.25], ['member', 0.4], ['non-european', 0.25], ['only', 0.25], ['members', 0.2], ['first', 0.15], ['joined', 0.25], ['becoming', 0.3], ['january', 0.3]]\n",
      "[['the', 0.02], ['based', 0.3], ['of', 0.05], ['on', 0.14], ['member', 0.28], ['states', 0.28], ['contributions', 0.36], ['their', 0.25], ['are', 0.15], ['budget', 0.42], ['computed', 0.42], ['gdp', 0.42]]\n",
      "[['(', 0.32], [')', 0.32], [',', 0.01], [':', 0.32], ['[', 0.03], [']', 0.03], ['as', 0.03], ['cern', 0.04], ['in', 0.03], ['is', 0.02], ['research', 0.02], ['the', 0.0], ['a', 0.01], ['of', 0.01], ['and', 0.03], ['currently', 0.03], ['membership', 0.04], ['states', 0.03], ['observer', 0.04], ['also', 0.05], ['countries', 0.07], ['from', 0.02], ['institutions', 0.04], ['members', 0.34], ['scientific', 0.03], ['at', 0.02], ['been', 0.03], ['have', 0.03], ['international', 0.07], ['other', 0.02], ['has', 0.05], ['with', 0.04], ['germany', 0.04], ['republic', 0.04], ['yugoslavia', 0.03], ['are', 0.05], ['such', 0.03], ['three', 0.04], ['spain', 0.11], ['-', 0.28], ['agreements', 0.04], ['aid', 0.05], ['animated', 0.05], ['associate', 0.05], ['austria', 0.05], ['borders', 0.51], ['bulgaria', 0.05], ['can', 0.04], ['candidates', 0.05], ['change', 0.05], ['changes', 0.05], ['co-operation', 0.05], ['contacts', 0.05], ['czech', 0.05], ['dates', 0.09], ['diplomacy', 0.05], ['finland', 0.05], ['following', 0.09], ['founded', 0.05], ['hungary', 0.05], ['involved', 0.05], ['join', 0.09], ['joins', 0.23], ['leaves', 0.09], ['map', 0.05], ['non-member', 0.05], ['observers', 0.04], ['organizations', 0.04], ['poland', 0.05], ['portugal', 0.05], ['post', 0.19], ['programmes', 0.05], ['re-joins', 0.05], ['reunified', 0.05], ['science', 0.03], ['showing', 0.05], ['slovakia', 0.05], ['status', 0.05], ['until', 0.05]]\n",
      "[['cern', 0.06], ['the', 0.01], ['world', 0.17], ['a', 0.08], ['of', 0.04], ['to', 0.06], ['through', 0.21], ['large', 0.22], ['current', 0.28], ['collaboration', 0.24], ['historical', 0.28], ['are', 0.12], ['around', 0.25], ['associated', 0.28], ['agreements', 0.28], ['and/or', 0.33], ['institutes', 0.33], ['links', 0.33], ['number', 0.28]]\n",
      "[[',', 0.01], [':', 0.09], ['an', 0.11], ['as', 0.07], ['cern', 0.11], ['is', 0.07], ['research', 0.09], ['the', 0.03], ['based', 0.14], ['on', 0.2], ['and', 0.09], ['observer', 0.15], ['scientific', 0.12], ['to', 0.07], ['through', 0.12], ['which', 0.09], ['council', 0.12], ['represented', 0.15], ['making', 0.17], ['observers', 0.17], ['organizations', 0.46], ['science', 0.12], ['accessible', 0.2], ['below', 0.2], ['contains', 0.2], ['creating', 0.2], ['focuses', 0.2], ['knowledge', 0.2], ['list', 0.2], ['model', 0.2], ['movement', 0.2], ['open', 0.18], ['openly', 0.2], ['processes', 0.2], ['tools', 0.17]]\n",
      "[[',', 0.06], ['cern', 0.04], ['in', 0.04], ['research', 0.1], ['of', 0.03], ['and', 0.11], ['data', 0.12], ['been', 0.14], ['which', 0.1], ['access', 0.12], ['has', 0.12], ['its', 0.13], ['are', 0.08], ['since', 0.14], ['science', 0.14], ['open', 0.53], ['areas', 0.23], ['components', 0.19], ['digital', 0.23], ['formation', 0.23], ['hardware', 0.23], ['licenses', 0.23], ['preservation', 0.16], ['primary', 0.23], ['reproducible', 0.19], ['software', 0.16], ['source', 0.19], ['towards', 0.17], ['working', 0.23]]\n",
      "[[',', 0.02], ['cern', 0.09], ['in', 0.05], ['that', 0.23], ['a', 0.06], ['of', 0.03], ['and', 0.08], ['official', 0.21], ['to', 0.04], [\"'s\", 0.11], ['which', 0.11], ['has', 0.13], ['convention', 0.17], ['its', 0.14], ['be', 0.14], ['with', 0.11], ['founding', 0.19], ['made', 0.21], ['results', 0.19], ['published', 0.19], ['or', 0.19], ['are', 0.09], ['starting', 0.21], ['all', 0.17], ['science', 0.15], ['number', 0.21], ['open', 0.11], ['available', 0.24], ['developed', 0.24], ['documents', 0.21], ['enable', 0.21], ['generally', 0.21], ['indicated', 0.24], ['policies', 0.21], ['promote', 0.24]]\n",
      "[['(', 0.06], [')', 0.06], [',', 0.06], ['an', 0.11], ['cern', 0.07], ['in', 0.04], ['that', 0.19], ['the', 0.01], ['and', 0.06], ['data', 0.11], ['collaborations', 0.17], ['main', 0.12], ['lhc', 0.07], ['which', 0.09], ['access', 0.22], ['by', 0.14], ['was', 0.06], ['its', 0.11], ['be', 0.11], ['with', 0.09], ['published', 0.31], ['recently', 0.17], ['then', 0.14], ['alice', 0.17], ['atlas', 0.16], ['cms', 0.16], ['lhcb', 0.17], ['since', 0.13], ['most', 0.14], ['all', 0.14], ['open', 0.28], ['authors', 0.2], ['endorsed', 0.2], ['ensures', 0.2], ['four', 0.2], ['gold', 0.2], ['policy', 0.27], ['publications', 0.2], ['will', 0.2]]\n",
      "[[',', 0.02], ['the', 0.03], ['a', 0.07], ['of', 0.04], ['scientific', 0.18], ['data', 0.3], ['experiments', 0.12], ['lhc', 0.1], ['access', 0.15], ['by', 0.1], ['after', 0.18], ['period', 0.2], ['open', 0.26], ['policy', 0.38], ['addressing', 0.24], ['collected', 0.29], ['complements', 0.29], ['embargo', 0.29], ['public', 0.24], ['release', 0.29], ['suitable', 0.29]]\n",
      "[[',', 0.04], ['for', 0.08], ['and', 0.04], ['to', 0.05], ['data', 0.28], ['through', 0.17], ['which', 0.12], ['access', 0.15], ['by', 0.09], ['this', 0.17], ['when', 0.18], ['were', 0.15], ['collaboration', 0.19], ['their', 0.16], ['prior', 0.23], ['each', 0.16], ['are', 0.1], ['necessary', 0.23], ['open', 0.13], ['preservation', 0.19], ['policies', 0.23], ['policy', 0.18], ['guidelines', 0.23], ['implemented', 0.27], ['individually', 0.27], ['own', 0.27], ['reuse', 0.27], ['updated', 0.23]]\n",
      "[[',', 0.03], [':', 0.07], ['cern', 0.03], ['european', 0.09], ['for', 0.17], ['in', 0.03], ['particle', 0.19], ['physics', 0.19], ['research', 0.07], ['that', 0.07], ['the', 0.06], ['a', 0.07], ['of', 0.05], ['on', 0.05], ['and', 0.05], ['to', 0.05], [\"'s\", 0.14], ['by', 0.1], ['europe', 0.13], ['was', 0.04], ['council', 0.09], ['organisation', 0.13], ['be', 0.08], ['with', 0.07], ['document', 0.1], ['then', 0.1], ['should', 0.25], ['work', 0.13], ['future', 0.13], ['within', 0.13], ['science', 0.28], ['open', 0.2], ['policy', 0.1], ['updated', 0.13], ['adopted', 0.15], ['affirmed', 0.15], ['authorities', 0.15], ['community', 0.13], ['consensus', 0.15], ['cornerstone', 0.15], ['decision-making', 0.15], ['emerging', 0.15], ['field', 0.15], ['forms', 0.15], ['help', 0.15], ['implement', 0.15], ['landscape', 0.15], ['last', 0.15], ['mandated', 0.15], ['publicly-funded', 0.15], ['relevant', 0.13], ['role', 0.15], ['shape', 0.15], ['stating', 0.15], ['strategy', 0.15], ['strongly', 0.15], ['“', 0.15], ['”', 0.15]]\n",
      "[[',', 0.04], ['cern', 0.12], ['in', 0.06], ['particle', 0.13], ['physics', 0.13], ['the', 0.01], ['a', 0.08], ['established', 0.25], ['of', 0.04], ['and', 0.15], ['more', 0.21], ['to', 0.06], ['at', 0.12], ['has', 0.17], ['level', 0.27], ['science', 0.2], ['open', 0.15], ['tools', 0.27], ['enable', 0.27], ['generally', 0.27], ['policy', 0.21], ['beyond', 0.32], ['guide', 0.32], ['services', 0.23], ['variety', 0.32]]\n",
      "[[',', 0.07], ['cern', 0.04], ['for', 0.07], ['in', 0.09], ['operates', 0.19], ['particle', 0.1], ['physics', 0.2], ['the', 0.02], ['a', 0.06], ['on', 0.08], ['and', 0.04], ['scientific', 0.15], ['to', 0.09], ['high-energy', 0.17], ['access', 0.26], ['has', 0.13], ['initiated', 0.21], ['project', 0.14], ['consortium', 0.21], ['side', 0.19], ['open', 0.23], ['articles', 0.21], ['convert', 0.21], ['cooperative', 0.24], ['global', 0.24], ['publishing', 0.49], ['scoap', 0.21], ['sponsoring', 0.24]]\n",
      "[[',', 0.02], ['in', 0.09], ['intergovernmental', 0.21], ['physics', 0.1], ['research', 0.11], ['the', 0.02], ['and', 0.04], ['currently', 0.15], ['countries', 0.17], ['from', 0.13], ['to', 0.09], ['have', 0.14], ['high-energy', 0.17], ['access', 0.13], ['worked', 0.21], ['represents', 0.21], ['across', 0.19], ['who', 0.21], ['organizations', 0.19], ['open', 0.11], ['articles', 0.21], ['convert', 0.21], ['scoap', 0.21], ['+', 0.25], ['collectively', 0.25], ['discipline', 0.25], ['journals', 0.25], ['leading', 0.25], ['libraries', 0.25], ['partnership', 0.25]]\n",
      "[[',', 0.1], [':', 0.08], ['as', 0.13], ['cern', 0.1], ['for', 0.05], ['physics', 0.07], ['the', 0.04], ['of', 0.02], ['on', 0.06], ['and', 0.05], ['used', 0.1], ['data', 0.18], ['at', 0.07], ['high-energy', 0.13], ['well', 0.15], ['researchers', 0.13], ['by', 0.12], ['be', 0.1], ['results', 0.14], ['their', 0.21], ['document', 0.13], ['are', 0.06], ['use', 0.15], ['various', 0.14], ['can', 0.15], ['open', 0.08], ['software', 0.13], ['documents', 0.15], ['community', 0.3], ['services', 0.25], ['case', 0.18], ['cern-based', 0.18], ['core', 0.18], ['depending', 0.18], ['etc', 0.18], ['hepdata', 0.18], ['inspire', 0.18], ['multimedia', 0.18], ['portal', 0.18], ['public-facing', 0.18], ['publication', 0.18], ['served', 0.18], ['server', 0.18], ['wider', 0.18], ['zenodo', 0.18]]\n",
      "[['(', 0.09], [')', 0.09], [',', 0.02], ['as', 0.1], ['cern', 0.05], ['physics', 0.12], ['research', 0.13], ['the', 0.01], ['a', 0.07], ['of', 0.03], ['and', 0.09], ['data', 0.14], [\"'s\", 0.13], ['computing', 0.19], ['by', 0.1], ['represented', 0.22], ['are', 0.1], ['entire', 0.24], ['such', 0.17], ['preservation', 0.2], ['reproducible', 0.24], ['software', 0.2], ['towards', 0.22], ['addressing', 0.24], ['services', 0.2], ['analysis', 0.24], ['best', 0.24], ['efforts', 0.28], ['environment', 0.28], ['lifecycle', 0.28], ['suite', 0.28]]\n",
      "[['(', 0.07], [')', 0.07], [';', 0.14], ['cern', 0.04], ['physics', 0.09], ['research', 0.09], ['the', 0.03], ['of', 0.05], ['on', 0.07], ['and', 0.03], ['to', 0.04], ['data', 0.11], ['researchers', 0.15], ['their', 0.13], ['document', 0.15], ['various', 0.16], ['components', 0.18], ['preservation', 0.15], ['analysis', 0.18], ['analyses', 0.63], ['cloud', 0.21], ['enables', 0.21], ['helps', 0.21], ['instantiating', 0.21], ['preserve', 0.21], ['preserved', 0.21], ['reana', 0.21], ['reusable', 0.21]]\n",
      "[[',', 0.04], ['as', 0.06], ['european', 0.11], ['the', 0.03], ['of', 0.02], ['and', 0.08], ['at', 0.06], ['by', 0.06], ['while', 0.13], ['with', 0.08], ['activities', 0.15], ['are', 0.06], ['into', 0.11], ['effort', 0.15], ['time', 0.13], ['built', 0.15], ['such', 0.11], ['same', 0.15], ['where', 0.3], ['all', 0.12], ['open', 0.08], ['software', 0.12], ['source', 0.15], ['towards', 0.13], ['guidelines', 0.15], ['relevant', 0.15], ['services', 0.12], ['best', 0.15], ['abovementioned', 0.17], ['account', 0.17], ['appropriate', 0.17], ['carried', 0.17], ['commission', 0.17], ['compliance', 0.17], ['fair', 0.17], ['force', 0.17], ['out', 0.17], ['plan', 0.17], ['possible', 0.17], ['principles', 0.35], ['s', 0.17], ['strive', 0.17], ['taking', 0.17], ['using', 0.17]]\n",
      "[['(', 0.08], [')', 0.08], [':', 0.12], ['as', 0.09], ['cern', 0.09], ['particle', 0.11], ['the', 0.03], ['and', 0.04], ['also', 0.15], ['to', 0.09], ['at', 0.1], ['facilities', 0.4], ['first', 0.13], ['include', 0.22], ['accelerator', 0.14], ['magnet', 0.22], ['such', 0.16], ['open', 0.12], ['public', 0.22], ['cerns', 0.26], ['certain', 0.26], ['daily', 0.26], ['provides', 0.26], ['superconducting', 0.26], ['synchro-cyclotron', 0.26], ['tours', 0.26], ['workshop', 0.26]]\n",
      "[[',', 0.07], ['cern', 0.06], ['in', 0.07], ['the', 0.04], ['a', 0.08], ['of', 0.09], ['at', 0.13], ['was', 0.1], ['-m', 0.35], ['dancing', 0.3], ['form', 0.35], ['god', 0.35], ['hindu', 0.35], ['nataraja', 0.35], ['shiva', 0.27], ['statue', 0.27], ['unveiled', 0.35]]\n",
      "[[',', 0.04], ['research', 0.12], ['the', 0.03], ['of', 0.03], ['and', 0.04], ['to', 0.05], [\"'s\", 0.25], ['by', 0.09], ['was', 0.07], ['with', 0.12], ['association', 0.23], ['creation', 0.23], ['shiva', 0.21], ['statue', 0.21], ['celebrate', 0.27], ['center', 0.27], ['cosmic', 0.19], ['dance', 0.19], ['destruction', 0.27], ['government', 0.27], ['india', 0.27], ['indian', 0.23], ['long', 0.27], ['presented', 0.27], ['symbolizing', 0.27]]\n",
      "[[',', 0.01], [':', 0.1], ['in', 0.04], ['the', 0.02], ['a', 0.1], ['of', 0.1], ['from', 0.11], ['to', 0.04], [\"'s\", 0.1], ['years', 0.16], ['with', 0.09], ['created', 0.18], ['next', 0.18], ['special', 0.16], ['dancing', 0.18], ['shiva', 0.16], ['statue', 0.16], ['cosmic', 0.15], ['dance', 0.15], ['indian', 0.18], ['ago', 0.21], ['artists', 0.21], ['beautiful', 0.21], ['bronzes', 0.21], ['capra', 0.21], ['explains', 0.21], ['fritjof', 0.21], ['hundreds', 0.21], ['images', 0.21], ['metaphor', 0.18], ['physicist', 0.21], ['plaque', 0.21], ['quotations', 0.21], ['series', 0.21], ['shivas', 0.21], ['visual', 0.21]]\n",
      "[[',', 0.02], ['in', 0.07], ['the', 0.04], ['of', 0.04], ['to', 0.06], ['used', 0.21], ['have', 0.2], ['technology', 0.27], ['physicists', 0.27], ['time', 0.27], ['most', 0.25], ['cosmic', 0.25], ['dance', 0.25], ['advanced', 0.35], ['our', 0.35], ['patterns', 0.35], ['portray', 0.35]]\n",
      "[[',', 0.02], ['physics', 0.14], ['the', 0.03], ['of', 0.04], ['and', 0.05], ['cosmic', 0.24], ['dance', 0.24], ['metaphor', 0.29], ['ancient', 0.33], ['art', 0.33], ['modern', 0.33], ['mythology', 0.33], ['religious', 0.33], ['thus', 0.33], ['unifies', 0.33]]\n",
      "[[':', 0.13], ['/', 0.24], ['coordinates', 0.28], ['°', 0.56], ['′', 0.56], ['″e\\ufeff', 0.28], ['″n', 0.28], ['\\ufeff', 0.28]]\n",
      "[['°n', 1.0]]\n",
      "[['/', 0.65], ['°e\\ufeff', 0.76]]\n",
      "[[';', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "#TFID similariry\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c55a1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity model\n",
    "sims = gensim.similarities.Similarity(r'C:\\Users\\k0nv1ct\\Documents\\DL Tasks -12-6-22\\NLP',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07a3095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 120\n"
     ]
    }
   ],
   "source": [
    "#DOcumnet 2 tokenization\n",
    "file2_docs = []\n",
    "\n",
    "\n",
    "tokens = sent_tokenize(doc2)\n",
    "for line in tokens:\n",
    "    file2_docs.append(line)\n",
    "\n",
    "print(\"Number of sentences:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and create bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d6b3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc51406d8814714b7d4dbe9f28245ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m], final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m      8\u001b[0m     sentence_pairs\u001b[38;5;241m.\u001b[39mappend([sentence1, sentence2])\n\u001b[1;32m---> 10\u001b[0m stsb_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSBERT CrossEncoder_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:274\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[1;34m(self, sentences, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_device)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    275\u001b[0m         model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfeatures, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    276\u001b[0m         logits \u001b[38;5;241m=\u001b[39m activation_fct(model_predictions\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:94\u001b[0m, in \u001b[0;36mCrossEncoder.smart_batching_collate_text_only\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(example):\n\u001b[1;32m---> 94\u001b[0m         texts[idx]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m())\n\u001b[0;32m     96\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;241m*\u001b[39mtexts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_first\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m tokenized:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "#using Sbert on wikipedia Articles\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "\n",
    "sentence_pairs = []\n",
    "for sentence1, sentence2 in zip(final['sentence1'], final['sentence2']):\n",
    "    sentence_pairs.append([sentence1, sentence2])\n",
    "    \n",
    "stsb_test['SBERT CrossEncoder_score'] = model.predict(sentence_pairs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af9d5351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.30735007 0.         0.         0.         0.17673723 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.15205844 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13998798 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# perform a similarity query against the corpus\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d68b192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "326fd81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.480111430088679"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Score(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "46e22a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score(lst):\n",
    "    return (sum(lst) / len(lst))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8c5858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for method 2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_processing(sentence):\n",
    "    \"\"\"\n",
    "    Lemmatize, lowercase, remove numbers and stop words\n",
    "    \n",
    "    Args:\n",
    "      sentence: The sentence we want to process.\n",
    "    \n",
    "    Returns:\n",
    "      A list of processed words\n",
    "    \"\"\"\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in nlp(sentence) \n",
    "                if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def cos_sim(sentence1_emb, sentence2_emb):\n",
    "    \"\"\"\n",
    "    Cosine similarity between two columns of sentence embeddings\n",
    "    \n",
    "    Args:\n",
    "      sentence1_emb: sentence1 embedding column\n",
    "      sentence2_emb: sentence2 embedding column\n",
    "    \n",
    "    Returns:\n",
    "      The row-wise cosine similarity between the two columns.\n",
    "      For instance is sentence1_emb=[a,b,c] and sentence2_emb=[x,y,z]\n",
    "      Then the result is [cosine_similarity(a,x), cosine_similarity(b,y), cosine_similarity(c,z)]\n",
    "    \"\"\"\n",
    "    cos_sim = cosine_similarity(sentence1_emb, sentence2_emb)\n",
    "    return np.diag(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2beee9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e047c4f2ba47a2a4c7c71c5053764b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#similarity using sbert crossencoder pretrained model\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "\n",
    "sentence_pairs = []\n",
    "for sentence1, sentence2 in zip(stsb_test['sentence1'], stsb_test['sentence2']):\n",
    "    sentence_pairs.append([sentence1, sentence2])\n",
    "    \n",
    "stsb_test['SBERT CrossEncoder_score'] = model.predict(sentence_pairs, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbbfcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAving the model as pickel object\n",
    "import pickle\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76069633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SBERT CrossEncoder_score</th>\n",
       "      <td>90.172556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          similarity_score\n",
       "SBERT CrossEncoder_score         90.172556"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SBERT cross encder and other methods results compared \n",
    "score_cols = [col for col in stsb_test.columns if '_score' in col]\n",
    "\n",
    "# Spearman Rank Correlation\n",
    "spearman_rank_corr = stsb_test[score_cols].corr(method='spearman').iloc[1:, 0:1]*100\n",
    "spearman_rank_corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7ff6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Train the model\n",
    "X_train = pd.concat([stsb_train['sentence1'], stsb_train['sentence2']]).unique()\n",
    "model.fit(X_train)\n",
    "\n",
    "# Generate Embeddings on Test\n",
    "sentence1_emb = model.transform(stsb_test['sentence1'])\n",
    "sentence2_emb = model.transform(stsb_test['sentence2'])\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['TFIDF_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdde476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2917010460ef49a0a3e2cf527dbd168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f788cacfcb47b8817786bf8375cf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7782db3f51f3435fbe04061279e870f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0934fb3562f74336bc9f16b4e3ed6f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968adf4e3f44466bf905d260856289c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df146573e3945d8914200124c695d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py:441\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py:522\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    521\u001b[0m cache_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    524\u001b[0m     amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[0;32m    525\u001b[0m ):  \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py:579\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 579\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py:544\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    536\u001b[0m                 \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    537\u001b[0m                 \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[0;32m    543\u001b[0m                 \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py:446\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [103]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstsb-mpnet-base-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate Embeddings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sentence1_emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(stsb_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m], show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     83\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# Download from hub with caching\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mignore_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflax_model.msgpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrust_model.ot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\util.py:491\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(huggingface_hub\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.8.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;66;03m# huggingface_hub v0.8.1 introduces a new cache layout. We sill use a manual layout\u001b[39;00m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;66;03m# And need to pass legacy_cache_layout=True to avoid that a warning will be printed\u001b[39;00m\n\u001b[0;32m    489\u001b[0m     cached_download_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy_cache_layout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m path \u001b[38;5;241m=\u001b[39m cached_download(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_download_args)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    494\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:737\u001b[0m, in \u001b[0;36mcached_download\u001b[1;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[0;32m    735\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 737\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, cache_path)\n\u001b[0;32m    746\u001b[0m os\u001b[38;5;241m.\u001b[39mreplace(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:490\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    481\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    482\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[0;32m    483\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    484\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[0;32m    489\u001b[0m )\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    492\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py:767\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;66;03m# Standard file-like object.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model2 = SentenceTransformer('stsb-mpnet-base-v2')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SBERT BiEncoder_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ba10230",
   "metadata": {},
   "outputs": [
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m########## Supervised ##########\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprinceton-nlp/sup-simcse-roberta-large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate Embeddings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sentence1_emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(stsb_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m], show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     83\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# Download from hub with caching\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mignore_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflax_model.msgpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrust_model.ot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\util.py:491\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(huggingface_hub\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.8.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;66;03m# huggingface_hub v0.8.1 introduces a new cache layout. We sill use a manual layout\u001b[39;00m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;66;03m# And need to pass legacy_cache_layout=True to avoid that a warning will be printed\u001b[39;00m\n\u001b[0;32m    489\u001b[0m     cached_download_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy_cache_layout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m path \u001b[38;5;241m=\u001b[39m cached_download(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_download_args)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    494\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:686\u001b[0m, in \u001b[0;36mcached_download\u001b[1;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m    681\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find the requested files in the cached path and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m outgoing traffic has been disabled. To enable model look-ups\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and downloads online, set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m                 )\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m    687\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection error, and we cannot find the requested files in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the cached path. Please try again or make sure your Internet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    689\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m connection is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    690\u001b[0m                 )\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# From now on, etag is not None.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cache_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "########## Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/sup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Supervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)\n",
    "\n",
    "\n",
    "########## Un-Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/unsup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Unsupervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb78522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
